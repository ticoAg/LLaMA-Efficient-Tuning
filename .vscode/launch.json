{
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "debug train rm",
            "type": "python",
            "request": "launch",
            "program": "src/train_bash.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--stage","rm",
                "--model_name_or_path","ticoAg/gpt2-tiger-sft-zh",
                "--overwrite_output_dir",
                "--do_train",
                "--finetuning_type","lora",
                "--lora_target", "c_proj",
                "--preprocessing_num_workers", "4",                
                "--dataset","comparison_gpt4_zh",
                "--num_train_epochs", "1",
                "--template","ziya",
                "--use_fast_tokenizer",
                "--per_device_train_batch_size","4",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps","4",
                "--output_dir",".cache/gpt2-tiger-zh-sft-rm",
                "--lr_scheduler_type","cosine",
                "--logging_steps","1",
                "--save_steps","0.5",
                "--warmup_ratio","0.1",
                "--learning_rate","1e-5",
                "--plot_loss",
                "--run_name", "gpt2-tiger-zh-sft-rm",
                "--bf16"
            ]
        },
        {
            "name": "debug train ppo",
            "type": "python",
            "request": "launch",
            "program": "src/train_bash.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--stage","ppo",
                "--model_name_or_path","ticoAg/gpt2-tiger-sft-zh",
                "--overwrite_output_dir",
                "--do_train",
                "--finetuning_type","full",
                // "--lora_target", "c_proj",
                "--preprocessing_num_workers", "4",                
                "--dataset","alpaca_gpt4_zh",
                "--num_train_epochs", "3",
                "--template","ziya",
                "--use_fast_tokenizer",
                "--per_device_train_batch_size","2",
                "--per_device_eval_batch_size", "2",
                "--gradient_accumulation_steps","1",
                "--reward_model", ".cache/gpt2-tiger-zh-sft-rm",
                "--output_dir",".cache/gpt2-tiger-zh-sft-ppo",
                "--lr_scheduler_type","cosine",
                "--logging_steps","1",
                "--save_steps","0.5",
                "--warmup_ratio","0.1",
                "--learning_rate","1e-5",
                "--run_name", "gpt2-tiger-zh-sft-ppo",
                "--plot_loss",
                "--bf16"
            ]
        }
    ]
}